{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "Study notes about Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, lets load Matplotlib and NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial neural networks\n",
    "\n",
    "### Perceptron neurons\n",
    "\n",
    "For historical reasons lets start with the simplest form of a neuron, the _Perceptron_. Developed by Frank Rosenblatt inspired by earlier works by Warren McCulloch and Walter Pitts. We do not use _Perceptrons_ neurons nowadays (_sigmoid_ neurons are much more common now) but it will give us and idea of the basic elements of an artificial neuron.\n",
    "\n",
    "A _Perceptron_ consists of many __binary__ inputs $x_1, x_2, x_3, ..., x_n$ and only one __binary__ output. It can be represented graphically by:\n",
    "\n",
    "![Perceptron neuron](http://i.imgur.com/AQzpxb7.png)\n",
    "\n",
    "Rosenblatt introduced the concept of _weights_: each _weight_ is a continuous value $w_i$ associated with each _input_ $x_i$. A _Perceptron_ __fires__ -- its output is 0 or 1 -- when the sum of its weights and inputs are higher than some given _threshold_, or:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\mbox{output} & = & \\left\\{ \\begin{array}{ll}\n",
    "      0 & \\mbox{if } \\sum_j w_j x_j \\leq \\mbox{ threshold} \\\\\n",
    "      1 & \\mbox{if } \\sum_j w_j x_j > \\mbox{ threshold}\n",
    "      \\end{array} \\right.\n",
    "\\tag{1}\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "We can implement this kind of neuron as the following, representing inputs and weights as vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, w, x, threshold):\n",
    "        self.w = w\n",
    "        self.x = x\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def run(self):\n",
    "        sum_ = 0\n",
    "        for j in xrange(0, len(self.w)):\n",
    "            sum_ += self.w[j] * self.x[j]\n",
    "        if sum_ <= self.threshold:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of a _Perceptron_ as a _decision machine_ where inputs are _factors_ we want to choose between and weights are _how much we value_ those factors or features.\n",
    "\n",
    "For example, let's imagine we want to buy a sofa, so we write down some factors about it:\n",
    "\n",
    "- Is it confortable? Yes.\n",
    "- Does it have the color you want? No.\n",
    "- Does it fit in your TV room? Yes.\n",
    "\n",
    "We can represent those factors as inputs, with answers \"Yes\" and \"No\" being represented as 1 or 0: $\\vec{x} = (1, 0, 1)$. To complement our description we can say how much we value each of those factors:\n",
    "\n",
    "- Is it confortable? I really want a cozy sofa!\n",
    "- Does it have the color your want? It's not a big deal.\n",
    "- Does it fit in your TV room? It's important, but I can left it on the social area, there's lots of space there.\n",
    "\n",
    "Let's represent those as weights: $\\vec{w} = (1.0, 0.2, 0.6)$. Note that's totally subjective but helps to give some organic/natural aspect of it.\n",
    "\n",
    "Given that we can run our _Perceptron_ and see what decision we should go for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron = Perceptron([1.0, 0.2, 0.6], [1, 0, 1], 0.5)\n",
    "neuron.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our _Perceptron_ says we should buy the sofa. See that the _threshold_ is also important: if we increase it, we can be more criterious about the features, if we decrease it, we will be more open to buy a sofa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron = Perceptron([1.0, 0.2, 0.6], [1, 0, 1], 1.8)\n",
    "neuron.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron = Perceptron([1.0, 0.2, 0.6], [1, 0, 1], 0.2)\n",
    "neuron.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Varying the __weights__ and __threshold__ we can get different _decision models_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine many neurons we want, creating _layers_ of _Perceptrons_:\n",
    "\n",
    "![MLP](http://i.imgur.com/ilgR4Cf.png)\n",
    "\n",
    "Each layer encapsulates a decision. Neurons on the right layer are more _high level_ than the neurons on the first layer because they decide about decisions already made by the first layer. Given so, the second layer is more _abstract_ than the first one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do some improvements on our _Perceptron_ model. First, we known from Linear Algebra that $\\vec{x} \\cdot \\vec{y} = \\sum_j x_j w_j$, so we can rewrite the previous equation as $\\vec{x} \\cdot \\vec{w} \\leq \\mbox{threshold}$. We can introduce the concept of _Perceptron's bias_ $b$ and represent threshold as $b \\equiv -\\mbox{threshold}$ and rewrite the inequality:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "    \\vec{x} \\cdot \\vec{w} &\\leq& \\mbox{threshold} \\\\\n",
    "    \\vec{x} \\cdot \\vec{w} -\\mbox{threshold} &\\leq& 0 \\\\\n",
    "    \\vec{x} \\cdot \\vec{w} + b &\\leq& 0 \\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "We should see the _bias_ as _how easy is to fire the neuron_ which makes it sounds more similar with the behavior or a natural neuron.\n",
    "\n",
    "Finally, we can rewrite the whole output function:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\mbox{output} & = & \\left\\{ \\begin{array}{ll}\n",
    "      0 & \\mbox{if } \\vec{x} \\cdot \\vec{w} + b \\leq 0 \\\\\n",
    "      1 & \\mbox{if } \\vec{x} \\cdot \\vec{w} + b > 0\n",
    "      \\end{array} \\right.\n",
    "\\tag{2}\\end{eqnarray}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can rewrite our _Perceptron_ class accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, w, x, bias):\n",
    "        self.w = w\n",
    "        self.x = x\n",
    "        self.bias = bias\n",
    "        \n",
    "    def run(self):\n",
    "        output = np.dot(self.x, self.w) + self.bias\n",
    "        if output <= 0:\n",
    "            return 0\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron = Perceptron([1.0, 0.2, 0.6], [1, 0, 1], 0.8)\n",
    "neuron.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do other minimal modification to our _Perceptron_ model, another more conceptual change. Note the output function is actually a _step function_. So let's make it clear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step(z):\n",
    "    if z <= 0:\n",
    "        return 0.\n",
    "    return 1.\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, w, x, bias):\n",
    "        self.w = w\n",
    "        self.x = x\n",
    "        self.bias = bias\n",
    "        \n",
    "    def run(self):\n",
    "        output = np.dot(self.x, self.w) + self.bias\n",
    "        return step(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid neurons\n",
    "\n",
    "A Sigmoid neuron is a type of neuron that uses a sigmoid function as \n",
    "an activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1. / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that Perceptron uses a step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step(z):\n",
    "    if z <= 0:\n",
    "        return 0.\n",
    "    return 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare both functions, we can see sigmoid is much smoother than step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEACAYAAABMEua6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0c6f17d690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 100\n",
    "xs = np.linspace(-10, 10, n)\n",
    "\n",
    "plt.plot(xs, sigmoid(xs), 'b-')\n",
    "plt.plot(xs, [step(x) for x in xs], 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using different activation functions $f(w . x + b)$ will make changes on\n",
    "the partial derivatives (that we use to change values of $w$ and $b$, smoothly).\n",
    "Because sigmoid function has a exponential, and exponentials have sweet\n",
    "properties while derivated, we're going to use them a lot as activation\n",
    "functions.\n",
    "\n",
    "Given that, let's create a Sigmoid neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self, w, x, bias):\n",
    "        self.w = np.array(w)\n",
    "        self.x = np.array(x)\n",
    "        self.bias = bias\n",
    "\n",
    "    def sigma(self, z):\n",
    "        sum_ = 0\n",
    "        for j in xrange(self.w.size):\n",
    "            sum_ += self.w[j] * self.x[j] - self.bias\n",
    "        return 1.0 / (1.0 + np.exp(-sum_))\n",
    "\n",
    "    def run(self):\n",
    "        return self.sigma(self.w.dot(self.x) + self.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0040701377159\n"
     ]
    }
   ],
   "source": [
    "sigmoid = Sigmoid([0.5, 0.5], [0, 1], 3.0)\n",
    "print sigmoid.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that we have a continuous value than a stiff 0 or 1, that opens room to learning (making small changes on values to tweak weights will give us sigma values with smooth changes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# References\n",
    "\n",
    "- [_Neural Networks and Deep Learning_](http://neuralnetworksanddeeplearning.com), Michael Nielsen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
